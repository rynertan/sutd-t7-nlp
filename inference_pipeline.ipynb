{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_combined_model import load_combined_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from d2l import torch as d2l \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_path = \"./weights/best_combined_model.pth\"\n",
    "test_file_path = \"./test_data_movie.csv\"\n",
    "output_csv = \"predictions.csv\"\n",
    "data_dir = \"aclImdb\"\n",
    "batch_size = 64\n",
    "num_steps = 500 \n",
    "combined_dim = 764  \n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "dropout_rate = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 49347\n"
     ]
    }
   ],
   "source": [
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    folder = \"train\" if is_train else \"test\"\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        label_dir = os.path.join(data_dir, folder, label)\n",
    "        for filename in os.listdir(label_dir):\n",
    "            with open(os.path.join(label_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                data.append(f.read())\n",
    "            labels.append(1 if label == \"pos\" else 0)\n",
    "    return data, labels\n",
    "\n",
    "def load_vocab(data_dir, num_steps):\n",
    "    \"\"\"Load vocabulary based on training data.\"\"\"\n",
    "    train_data, _ = read_imdb(data_dir, is_train=True)\n",
    "    train_tokens = d2l.tokenize(train_data, token=\"word\")\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=[\"<pad>\"])\n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab(data_dir, num_steps)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined model successfully loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonec\\code\\nlp4submit\\load_combined_model.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_model.load_state_dict(torch.load(combined_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "combined_model = load_combined_model(\n",
    "    combined_model_path, vocab_size, combined_dim, hidden_dim, output_dim, dropout_rate, device\n",
    ")\n",
    "print(\"Combined model successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_predictions(model, test_file_path, vocab, device, output_csv):\n",
    "    \"\"\"\n",
    "    Evaluate the combined model on the provided test set and save predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        model: Combined PyTorch model to evaluate.\n",
    "        test_file_path: Path to the test dataset CSV file (text, label).\n",
    "        vocab: Vocabulary object to tokenize the text data.\n",
    "        device: Device (CPU or GPU) to run the evaluation.\n",
    "        output_csv: Filename for the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary containing accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "    texts = test_data['text'].tolist()\n",
    "    labels = test_data['label'].tolist()\n",
    "\n",
    "    tokenized_texts = [vocab[token] for token in d2l.tokenize(texts, token='word')]\n",
    "    max_len = 500  \n",
    "    features = torch.tensor([d2l.truncate_pad(tokens, max_len, vocab['<pad>']) for tokens in tokenized_texts])\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(features, labels_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    all_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions = torch.argmax(outputs, axis=1).cpu().numpy()\n",
    "            y_pred.extend(predictions)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "            start_idx = batch_idx * test_loader.batch_size\n",
    "            end_idx = start_idx + X_batch.size(0)\n",
    "            batch_texts = texts[start_idx:end_idx]\n",
    "            all_texts.extend(batch_texts)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"binary\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"binary\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "\n",
    "    label_map = {0: 'negative', 1: 'positive'}\n",
    "    predicted_labels = [label_map[label] for label in y_pred]\n",
    "    true_labels = [label_map[label] for label in y_true]\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': all_texts,\n",
    "        'predicted': predicted_labels,\n",
    "        'truth': true_labels\n",
    "    })\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nPredictions saved to {output_csv}\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.93      0.94     20019\n",
      "    Positive       0.93      0.95      0.94     19981\n",
      "\n",
      "    accuracy                           0.94     40000\n",
      "   macro avg       0.94      0.94      0.94     40000\n",
      "weighted avg       0.94      0.94      0.94     40000\n",
      "\n",
      "\n",
      "Predictions saved to predictions.csv\n",
      "\n",
      "Combined Model Evaluation Metrics:\n",
      "Accuracy: 0.9405\n",
      "Precision: 0.9300\n",
      "Recall: 0.9526\n",
      "F1 Score: 0.9411\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = evaluate_and_save_predictions(combined_model, test_file_path, vocab, device, output_csv)\n",
    "\n",
    "print(\"\\nCombined Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
